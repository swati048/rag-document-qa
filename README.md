# ğŸ“š RAG Document Q&A System

A **free, cloud-powered** AI document question-answering system using Retrieval Augmented Generation (RAG). Upload PDFs or text files and ask questions in natural language!

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.31+-red.svg)
![LangChain](https://img.shields.io/badge/LangChain-Latest-yellow.svg)
![Groq](https://img.shields.io/badge/Groq-Cloud%20LLM-orange.svg)
![License](https://img.shields.io/badge/License-MIT-purple.svg)

**An intelligent document Q&A system powered by RAG (Retrieval-Augmented Generation)**

[Live Demo](#) â€¢ [Documentation](#quick-start) â€¢ [Report Bug](../../issues) â€¢ [Request Feature](../../issues)

</div>

## ğŸ“¸ Demo

![App Demo](screenshots/demo.gif)

## ğŸ¯ What is This?

Ever wished you could chat with your PDFs? This is a **production-ready RAG (Retrieval-Augmented Generation)** system that lets you:

- ğŸ“„ **Upload** any PDF or text document
- ğŸ’¬ **Ask questions** in natural language
- ğŸ¯ **Get accurate answers** grounded in your documents
- ğŸ“š **Source Citations** - know exactly where each answer comes from
- ğŸ” **Filter by file** - search specific documents by name
- ğŸ’¾ **Persistent storage** - your documents and chat history survive restarts

**Perfect for**: Research papers, legal documents, manuals, reports, study materials, or any text-heavy content you need to understand quickly.

## âœ¨ Key Features

### ğŸ§  **Intelligent Q&A**
- Ask questions in natural language
- Context-aware responses from Llama 3.3 70B via Groq
- Automatic source citation with chunk references

### ğŸ¯ **Smart Document Filtering**
- Ask "What is in report.pdf?" to search only that file
- Or search across all documents simultaneously
- Automatic filename detection in queries

### ğŸ’¡ **Developer-Friendly**
- **100% Free** - Groq free tier (30 requests/min)
- **No GPU needed** - Embeddings run on CPU
- **Fast setup** - 5 minutes from clone to running
- **Clean architecture** - FastAPI backend + Streamlit frontend

### ğŸ“Š **Production Features**
- Persistent vector storage (FAISS)
- Chat history with timestamps
- Document management (upload/delete/list)
- Error handling and rate limiting
- Health checks and monitoring endpoints

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "Frontend - Streamlit"
        A[User Interface]
        B[File Uploader]
        C[Chat Interface]
    end
    
    subgraph "Backend - FastAPI"
        D[API Endpoints]
        E[Document Manager]
        F[RAG Engine]
    end
    
    subgraph "Processing Pipeline"
        G[PDF/TXT Parser]
        H[Text Splitter<br/>1500 chars]
        I[Local Embeddings<br/>HuggingFace]
        J[FAISS Vector Store]
    end
    
    subgraph "LLM Layer"
        K[Query Embeddings]
        L[Similarity Search<br/>Top 9 chunks]
        M[Groq Cloud API<br/>Llama 3.3 70B]
    end
    
    A --> B
    A --> C
    B --> D
    C --> D
    D --> E
    D --> F
    E --> G
    G --> H
    H --> I
    I --> J
    F --> K
    K --> L
    L --> J
    L --> M
    M --> C
    
    style M fill:#ff9800
    style J fill:#4caf50
    style I fill:#2196f3
```

### ğŸ”„ How It Works

1. **Document Upload** â†’ PDF/TXT parsed â†’ Split into 1500-char chunks
2. **Embedding** â†’ Each chunk embedded using HuggingFace (local, free)
3. **Storage** â†’ Embeddings stored in FAISS vector database
4. **Query** â†’ User question embedded â†’ Find top 9 similar chunks
5. **Generation** â†’ Groq (Llama 3.3 70B) generates answer from chunks
6. **Response** â†’ Answer + source citations returned to user

## ğŸ› ï¸ Tech Stack

| Component | Technology | Why? |
|-----------|-----------|------|
| **Backend** | FastAPI | Fast, modern, async Python framework |
| **Frontend** | Streamlit | Rapid prototyping, beautiful UI out-of-the-box |
| **LLM** | Groq (Llama 3.3 70B) | Fastest inference, free tier, excellent quality |
| **Embeddings** | HuggingFace MiniLM | Local, free, no API costs |
| **Vector DB** | FAISS | Fast similarity search, persistent storage |
| **Orchestration** | LangChain | RAG pipeline management |
| **Document Parsing** | PyPDF | Reliable PDF text extraction |

## ğŸ“‹ Prerequisites

Before you begin, ensure you have:

- âœ… **Python 3.11+** installed
- âœ… **Groq API Key** (free) - [Get it here](https://console.groq.com)
- âœ… **~2GB disk space** for dependencies
- âœ… **Basic terminal/command line knowledge**

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/swati048/rag-document-qa.git
cd rag-document-qa
```

### 2ï¸âƒ£ Get Your Free Groq API Key

1. Visit [console.groq.com](https://console.groq.com)
2. Sign up (free) with Google/GitHub
3. Navigate to "API Keys" â†’ "Create API Key"
4. Copy your key (starts with `gsk_...`)

### 3ï¸âƒ£ Set Up Environment

```bash
# Create virtual environment
python -m venv venv

# Activate it
# On Mac/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure API Key

Create a `.env` file in the project root:

```bash
# .env
GROQ_API_KEY=gsk_your_actual_api_key_here
```

âš ï¸ **Important**: Never commit this file! It's already in `.gitignore`.

### 5ï¸âƒ£ Run the Application

**Terminal 1 - Backend:**
```bash
cd backend
python main.py
```
Backend runs at `http://localhost:8000`

**Terminal 2 - Frontend:**
```bash
cd frontend
streamlit run app.py
```
Frontend opens at `http://localhost:8501`

### 6ï¸âƒ£ Start Chatting!

1. Upload a PDF or TXT file in the sidebar
2. Wait for indexing to complete (~10-30 seconds)
3. Ask questions in the chat interface
4. Enjoy AI-powered answers with source citations! ğŸ‰

## ğŸ“š Usage Examples

### Basic Questions
```
â“ "What is the main topic of this document?"
â“ "Summarize the key findings"
â“ "What are the conclusions?"
â“ "Who are the authors mentioned?"
```

### File-Specific Queries
```
â“ "What is in research_paper.pdf?"
â“ "Summarize report.txt"
â“ "What does contract.pdf say about payment terms?"
```

### Advanced Queries
```
â“ "Compare the methodologies in section 2 and section 4"
â“ "What recommendations are mentioned in the conclusion?"
â“ "List all the statistics about climate change"
```

## ğŸ¨ Project Structure

```
rag-document-qa/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py              # Empty init file
â”‚   â”œâ”€â”€ config.py                # Configuration & API keys
â”‚   â”œâ”€â”€ document_manager.py      # Upload, delete, list docs
â”‚   â”œâ”€â”€ rag_engine.py           # RAG logic with Groq & FAISS
â”‚   â””â”€â”€ main.py                 # FastAPI app & endpoints
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                  # Streamlit UI
â”‚
â”œâ”€â”€ data/                       # Created automatically
â”‚   â”œâ”€â”€ uploads/                # Uploaded documents
â”‚   â”œâ”€â”€ vectorstore/            # FAISS index
â”‚   â””â”€â”€ chat_history.json       # Persisted conversations
â”‚
â”œâ”€â”€ .env                        # API keys (YOU CREATE THIS)
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## âš™ï¸ Configuration

Edit `backend/config.py` to customize:

```python
# LLM Model Selection
GROQ_MODEL = "llama-3.3-70b-versatile"  # Best quality (default)
# GROQ_MODEL = "llama-3.1-8b-instant"   # Faster responses

# Document Chunking
CHUNK_SIZE = 1500              # Characters per chunk
CHUNK_OVERLAP = 300            # Overlap between chunks

# Retrieval Settings
TOP_K_RESULTS = 9              # Number of chunks to retrieve

# Embeddings (local - no cost)
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
```

## ğŸ’° Cost & Limits

### Groq Free Tier
- âœ… **30 requests per minute**
- âœ… **14,400 tokens per minute**
- âœ… **No credit card required**
- âœ… **Access to Llama 3.3 70B**

**Models Available:**
- `llama-3.1-70b-versatile` â­ (default - best quality)
- `llama-3.1-8b-instant` (faster)
- `mixtral-8x7b-32768` (good balance)

### Cost Breakdown
| Operation | API Calls | Cost |
|-----------|-----------|------|
| Upload document | 0 | FREE (local embeddings) |
| Each question | 1 | FREE (within limits) |
| **Daily usage** | ~100-200 | **100% FREE** âœ… |

**Typical Usage**: 100-200 questions per day = completely free!

## ğŸŒ Deployment

### Streamlit Cloud (Frontend) + Render (Backend)

**Pros**: 
- âœ… Free hosting for both
- âœ… Auto-restarts on code push
- âœ… HTTPS by default
- âœ… Easy to manage

**Cons**:
- âš ï¸ Render free tier sleeps after 15 min inactivity (30s cold start)
- âš ï¸ Streamlit Cloud has resource limits

## ğŸ§ª Testing the API

### Health Check
```bash
curl http://localhost:8000/health
```

### Upload Document
```bash
curl -X POST http://localhost:8000/upload \
  -F "file=@document.pdf"
```

### Ask Question
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is this document about?"}'
```

## ğŸ› Troubleshooting

### "GROQ_API_KEY not found"
```bash
# Check .env file exists
ls -la .env

# Verify it contains:
GROQ_API_KEY=gsk_...
```

### "Backend not reachable"
```bash
# Test backend
curl http://localhost:8000/health

# Should return JSON
```

### "Rate limit exceeded"
- Wait 60 seconds
- You've hit 30 requests/minute limit
- Consider switching to `llama-3.1-8b-instant` in config

### Slow first query
- First query loads embedding model (~5-10s)
- Subsequent queries are faster (1-3s)
- This is normal!

### Documents not indexing
- Check file size (PDFs >50MB may timeout)
- Check backend logs for errors
- Ensure GROQ_API_KEY is valid

## ğŸ”’ Security Best Practices

1. âœ… **Never commit `.env`** - Already in `.gitignore`
2. âœ… **Rotate API keys** if exposed
3. âœ… **Use environment variables** in production
4. âœ… **Enable HTTPS** for production deployments
5. âœ… **Add authentication** if handling sensitive documents

## ğŸš€ Future Enhancements

- [ ] Support DOCX, XLSX, CSV files
- [ ] Multi-language support
- [ ] User authentication
- [ ] Document comparison mode
- [ ] Export chat history
- [ ] OCR for scanned PDFs
- [ ] Advanced filters (date, author, tags)
- [ ] Batch processing
- [ ] Vector store backup/restore

**Want to contribute?** Open an issue or PR!

## ğŸ“Š Performance Benchmarks

| Metric | Value |
|--------|-------|
| First query (cold start) | 5-10s |
| Subsequent queries | 1-3s |
| Document upload (10 pages) | 15-30s |
| Embedding speed | ~1000 chars/sec |
| Vector search | <100ms |
| Max file size tested | 100MB PDF |

## ğŸ¤ Contributing

This is a portfolio project, but suggestions are welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [LangChain](https://langchain.com) - RAG orchestration framework
- [Groq](https://groq.com) - Lightning-fast LLM inference
- [FAISS](https://github.com/facebookresearch/faiss) - Efficient vector search
- [Streamlit](https://streamlit.io) - Beautiful UI framework
- [HuggingFace](https://huggingface.co) - Free local embeddings
- [FastAPI](https://fastapi.tiangolo.com) - Modern Python web framework

## ğŸ‘¤ Author

**Swati Thakur**
- GitHub: [@swati048](https://github.com/swati048)
- LinkedIn: [Swati Thakur](https://linkedin.com/in/swati048)
- Email: [thakurswati048@gmail.com](mailto:thakurswati048@gmail.com)

---

<div align="center">

**â­ Star this repository if you found it helpful! â­**

**Built with â¤ï¸ using Groq's free tier**

</div>